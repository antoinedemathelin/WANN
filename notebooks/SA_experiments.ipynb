{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WANN experiments on Sentiment Analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.constraints import MinMaxNorm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "sys.path.append(\"../wann\")\n",
    "sys.path.append(\"../wann/methods\")\n",
    "from utils import sa\n",
    "from WANN import WANN\n",
    "\n",
    "from adapt.instance_based import KLIEP, KMM, TrAdaBoostR2\n",
    "from adapt.feature_based import MDD, DANN, ADDA, DeepCORAL\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_model(shape=1000, activation=None, C=1, name=\"BaseModel\"):\n",
    "    inputs = Input(shape=(shape,))\n",
    "    modeled = Dense(100, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(inputs)\n",
    "    modeled = Dropout(0.8)(modeled)\n",
    "    modeled = Dense(100, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(modeled)\n",
    "    modeled = Dropout(0.5)(modeled)\n",
    "    modeled = Dense(1, activation=activation,\n",
    "                    kernel_constraint=MinMaxNorm(0, C),\n",
    "                    bias_constraint=MinMaxNorm(0, C))(modeled)\n",
    "    model = Model(inputs, modeled, name=name)\n",
    "    model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_encoder(shape=1000, C=1, name=\"encoder\"):\n",
    "    inputs = Input(shape=(shape,))\n",
    "    modeled = Dense(100, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(inputs)\n",
    "    modeled = Dropout(0.8)(modeled)\n",
    "    model = Model(inputs, modeled)\n",
    "    model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_task(shape=100, C=1, activation=None, name=\"task\"):\n",
    "    inputs = Input(shape=(shape,))\n",
    "    modeled = Dense(100, activation='relu',\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(inputs)\n",
    "    modeled = Dropout(0.5)(modeled)\n",
    "    modeled = Dense(1, activation=activation,\n",
    "                         kernel_constraint=MinMaxNorm(0, C),\n",
    "                         bias_constraint=MinMaxNorm(0, C))(modeled)\n",
    "    model = Model(inputs, modeled)\n",
    "    model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "seeds = np.random.choice(2**16, 10)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "\n",
    "N = 50   # Number of labeled target data\n",
    "m = 700  # Number of labeled source data\n",
    "n = 700  # Number of unlabeled target data\n",
    "\n",
    "\n",
    "for source in [\"books\", \"electronics\", \"kitchen\", \"dvd\"]:\n",
    "    for target in [\"books\", \"electronics\", \"kitchen\", \"dvd\"]:\n",
    "        if source != target:\n",
    "            \n",
    "            print(source, target)\n",
    "            \n",
    "            scores = {}\n",
    "            scores_cv = {}\n",
    "\n",
    "            for i in [2,3,4,5,6,7,8,9,1,0]:\n",
    "\n",
    "                np.random.seed(seeds[i])\n",
    "                tf.random.set_seed(seeds[i])\n",
    "                \n",
    "                X, y, src_index, tgt_index = sa(source, target)\n",
    "                mu = 3.; std = 1.5\n",
    "                y = (y - 3.) / 1.5\n",
    "                shape = X.shape[1]\n",
    "\n",
    "                src_index = np.random.choice(src_index, m, replace=False)\n",
    "                tgt_index, tgt_test_index = train_test_split(tgt_index, train_size=n, test_size=1000)\n",
    "                tgt_index_labeled = np.random.choice(tgt_index, N, replace=False)\n",
    "                train_index = np.concatenate((src_index, tgt_index_labeled))\n",
    "                \n",
    "                test_ = np.random.choice(tgt_index_labeled, 10, replace=False)\n",
    "                train_ = np.array(list(set(tgt_index_labeled) - set(test_)))\n",
    "                \n",
    "                train_index_cv = np.concatenate((src_index, train_))\n",
    "\n",
    "                for method in [\"WANN\", \"TgtOnly\", \"NoReweight\", \"KLIEP\", \"KMM\", \"DANN\", \"ADDA\", \"DeepCORAL\", \"MDD\", \"TrAdaBoostR2\"]:\n",
    "                    print(method)\n",
    "                    if not method in scores:\n",
    "                        scores[method] = []\n",
    "                        if method in [\"WANN\", \"KLIEP\", \"KMM\", \"DANN\", \"DeepCORAL\", \"MDD\"]:\n",
    "                            scores_cv[method] = []\n",
    "                    \n",
    "                    cv_scores = []\n",
    "                    \n",
    "                    if method in [\"TgtOnly\", \"NoReweight\"]:\n",
    "                        model = get_base_model(shape=X.shape[1])\n",
    "\n",
    "                        if method == \"TgtOnly\":\n",
    "                            model.fit(X[tgt_index_labeled], y[tgt_index_labeled], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                        if method == \"NoReweight\":\n",
    "                            model.fit(X[train_index], y[train_index], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "                    elif method in [\"WANN\"]:\n",
    "                        for C_w in [0.1, 0.2, 0.5, 1.]:\n",
    "                            model = WANN(get_base_model=get_base_model, C=1., C_w=C_w, optimizer=Adam(0.001))   \n",
    "                            model.fit(X, y, [train_index, train_], epochs=epochs, verbose=0, batch_size=batch_size)\n",
    "                            err_t = np.mean(np.square(model.predict(X).ravel()[test_] - y[test_]))\n",
    "                            print(\"%.3f, target %.3f\"%(C_w, err_t))\n",
    "                            cv_scores.append(err_t)\n",
    "                        scores_cv[method].append(cv_scores)\n",
    "                        arg = np.argmin(cv_scores)\n",
    "                        C_w = [0.1, 0.2, 0.5, 1.][arg]\n",
    "                        model = WANN(get_base_model=get_base_model, C=1., C_w=C_w, optimizer=Adam(0.001))\n",
    "                        model.fit(X, y, [train_index, tgt_index_labeled], epochs=epochs, verbose=0, batch_size=batch_size)\n",
    "                        \n",
    "                    elif method == \"TrAdaBoostR2\":\n",
    "                        model = TrAdaBoostR2(get_base_model(), verbose=2, random_state=seeds[i])\n",
    "                        model.fit(X[train_index], y[train_index], X[tgt_index_labeled], y[tgt_index_labeled],\n",
    "                                  epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "                    else:            \n",
    "                        if method == \"DANN\":\n",
    "                            for lambda_ in [0.0001, 0.001, 0.01, 0.1, 1.]:\n",
    "                                model = DANN(encoder=get_encoder(), task=get_task(), random_state=seeds[i],\n",
    "                                             discriminator=get_task(activation=\"sigmoid\"),\n",
    "                                             optimizer=Adam(0.001), lambda_=lambda_, loss=\"mse\")\n",
    "                                model.fit(X[train_index], y[train_index], X[tgt_index], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                                err_t = np.mean(np.square(model.predict(X).ravel()[test_] - y[test_]))\n",
    "                                print(\"%.3f, target %.3f\"%(lambda_, err_t))\n",
    "                                cv_scores.append(err_t)\n",
    "                            scores_cv[method].append(cv_scores)\n",
    "                            arg = np.argmin(cv_scores)\n",
    "                            lambda_ = [0.0001, 0.001, 0.01, 0.1, 1.][arg]\n",
    "                            model = DANN(encoder=get_encoder(), task=get_task(), random_state=seeds[i],\n",
    "                                             discriminator=get_task(activation=\"sigmoid\"),\n",
    "                                             optimizer=Adam(0.001), lambda_=lambda_, loss=\"mse\")\n",
    "                        if method == \"DeepCORAL\":\n",
    "                            for lambda_ in [0.1, 1., 10, 100., 1000.]:\n",
    "                                model = DeepCORAL(encoder=get_encoder(), task=get_task(), lambda_=lambda_,\n",
    "                                                  optimizer=Adam(0.001), loss=\"mse\",\n",
    "                                                  random_state=seeds[i])\n",
    "                                model.fit(X[train_index], y[train_index], X[tgt_index], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                                err_t = np.mean(np.square(model.predict(X).ravel()[test_] - y[test_]))\n",
    "                                print(\"%.3f, target %.3f\"%(lambda_, err_t))\n",
    "                                cv_scores.append(err_t)\n",
    "                            scores_cv[method].append(cv_scores)\n",
    "                            arg = np.argmin(cv_scores)\n",
    "                            lambda_ = [0.1, 1., 10, 100., 1000.][arg]\n",
    "                            model = DeepCORAL(encoder=get_encoder(), task=get_task(), lambda_=lambda_,\n",
    "                                                  optimizer=Adam(0.001), loss=\"mse\",\n",
    "                                                  random_state=seeds[i])\n",
    "                        if method == \"MDD\":\n",
    "                            for lambda_ in [0.0001, 0.001, 0.01, 0.1, 1.]:\n",
    "                                model = MDD(encoder=get_encoder(), task=get_task(), random_state=seeds[i],\n",
    "                                            optimizer=Adam(0.001), lambda_=lambda_, loss=\"mse\")\n",
    "                                model.fit(X[train_index], y[train_index], X[tgt_index], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                                err_t = np.mean(np.square(model.predict(X).ravel()[test_] - y[test_]))\n",
    "                                print(\"%.3f, target %.3f\"%(lambda_, err_t))\n",
    "                                cv_scores.append(err_t)\n",
    "                            scores_cv[method].append(cv_scores)\n",
    "                            arg = np.argmin(cv_scores)\n",
    "                            lambda_ = [0.0001, 0.001, 0.01, 0.1, 1.][arg]\n",
    "                            model = MDD(encoder=get_encoder(), task=get_task(), random_state=seeds[i],\n",
    "                                            optimizer=Adam(0.001), lambda_=lambda_, loss=\"mse\")\n",
    "                        if method == \"ADDA\":\n",
    "                            encoder = get_encoder()\n",
    "                            task=get_task()\n",
    "                            discriminator=get_task(activation=\"sigmoid\")\n",
    "                            dann = DANN(encoder, task, discriminator, loss=\"mse\", copy=False,\n",
    "                                        lambda_=0., random_state=seeds[i])\n",
    "                            dann.fit(X[train_index], y[train_index], X[tgt_index], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                            model = ADDA(encoder=encoder, task=task,\n",
    "                                         discriminator=discriminator, random_state=seeds[i],\n",
    "                                         is_pretrained=True,\n",
    "                                         optimizer=Adam(0.001), loss=\"mse\")\n",
    "                        if method == \"KLIEP\":\n",
    "                            for sigmas in [0.0001, 0.001, 0.01, 0.1, 1.]:\n",
    "                                model = KLIEP(get_base_model(), sigmas=sigmas, random_state=seeds[i])\n",
    "                                model.fit(X[train_index], y[train_index], X[tgt_index], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                                err_t = np.mean(np.square(model.predict(X).ravel()[test_] - y[test_]))\n",
    "                                print(\"%.3f, target %.3f\"%(sigmas, err_t))\n",
    "                                cv_scores.append(err_t)\n",
    "                            scores_cv[method].append(cv_scores)\n",
    "                            arg = np.argmin(cv_scores)\n",
    "                            sigmas = [0.0001, 0.001, 0.01, 0.1, 1.][arg]\n",
    "                            model = KLIEP(get_base_model(), sigmas=sigmas, random_state=seeds[i])\n",
    "                        if method == \"KMM\":\n",
    "                            for sigmas in [0.0001, 0.001, 0.01, 0.1, 1.]:\n",
    "                                model = KMM(get_base_model(), kernel_params=dict(gamma=sigmas), verbose=0, random_state=seeds[i])\n",
    "                                model.fit(X[train_index], y[train_index], X[tgt_index], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                                err_t = np.mean(np.square(model.predict(X).ravel()[test_] - y[test_]))\n",
    "                                print(\"%.3f, target %.3f\"%(sigmas, err_t))\n",
    "                                cv_scores.append(err_t)\n",
    "                            scores_cv[method].append(cv_scores)\n",
    "                            arg = np.argmin(cv_scores)\n",
    "                            sigmas = [0.0001, 0.001, 0.01, 0.1, 1.][arg]\n",
    "                            model = KMM(get_base_model(), kernel_params=dict(gamma=sigmas), verbose=0, random_state=seeds[i])\n",
    "                            \n",
    "                        model.fit(X[train_index], y[train_index], X[tgt_index], epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                    \n",
    "                    err_s = np.mean(np.square(model.predict(X).ravel()[src_index] - y[src_index]))\n",
    "                    err_t = np.mean(np.square(model.predict(X).ravel()[tgt_test_index] - y[tgt_test_index]))\n",
    "\n",
    "                    scores[method].append(err_t)\n",
    "\n",
    "                    print(\"source %.3f\"%err_s)\n",
    "                    print(\"target %.3f\"%err_t)\n",
    "\n",
    "\n",
    "                pd.DataFrame(scores_cv).to_csv(\"../dataset/results/amazon_cv_%s%s.csv\"%(source, target))\n",
    "                pd.DataFrame(scores).to_csv(\"../dataset/results/amazon_goodparams_%s%s.csv\"%(source, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wann",
   "language": "python",
   "name": "wann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
